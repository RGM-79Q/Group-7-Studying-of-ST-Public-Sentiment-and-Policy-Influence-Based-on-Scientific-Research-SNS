{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             content\n",
      "0                      这教授还自以为他很自豪的感觉，难道是救世主能为全人类负责？\n",
      "1                                           对所有人类负责？\n",
      "2  7对夫妇，不止这两个孩子吧！别人的人生，别人的苦难，你怎么负责怎么替代？看看网上的那份协议书...\n",
      "3  我倒是很期待这个项目有多成功，会给人类带来多大的改变，奈何这个负责人太急功近利，没有几代的实...\n",
      "4  贺所谓的负责，是把这两个孩子一直作为实验对象和个人资产，好后续持续跟踪、观察、做试验以不断获...\n",
      "1202\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import re                                #正则表达式，可用于匹配中文文本\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"一级评论3.csv\", encoding='utf-8') \n",
    "\n",
    "\n",
    "print(df.head())\n",
    "print(len(df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1202\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna() #去除空值所在的整条(行)数据\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建停用词list\n",
    "def stopwordslist(filepath):\n",
    "    stopwords = [line.strip()for line in open(filepath,'r',encoding='utf-8').readlines()]\n",
    "    return stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对句子去除停用词\n",
    "def movestopwords(sentence):\n",
    "    stopwords = stopwordslist('scu_stopwords.txt')\n",
    "    outstr = ''\n",
    "    for word in sentence:\n",
    "        if word not in stopwords:\n",
    "            if word != '\\t'and'\\n':\n",
    "                outstr += word # outstr += \" \"\n",
    "    return outstr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词函数\n",
    "def chinese_word_cut(mytext):\n",
    "    import jieba.posseg as psg\n",
    "    checkarr = ['n']\n",
    "    returnmsg = \"\".join([(x.word) for x in psg.cut(mytext) if (x.flag in checkarr)])\n",
    "    return returnmsg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.720 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0               教授自豪感觉救世主全人类\n",
      "1                         人类\n",
      "2          夫妇孩子人生替代协议书科学底线人信\n",
      "3       项目人类负责人风险人类强子对撞机产生地球\n",
      "4               孩子对象个人资产研究进展\n",
      "                ...         \n",
      "1197                     全人类\n",
      "1198                      责任\n",
      "1199                      孩子\n",
      "1200                        \n",
      "1201                        \n",
      "Name: content, Length: 1202, dtype: object\n"
     ]
    }
   ],
   "source": [
    " # 分词结果  \n",
    "df = df.content.apply(chinese_word_cut)\n",
    "ff = df.apply(movestopwords)\n",
    "ff = ff.dropna()\n",
    "\n",
    "print(ff) #预览"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('人', 25), ('人类', 16), ('孩子', 16), ('责任', 12), ('全人类', 10), ('责', 9), ('人生', 7), ('国家', 7), ('监狱', 6)]\n"
     ]
    }
   ],
   "source": [
    "word_counts = collections.Counter(ff) # 对分词做词频统计\n",
    "\n",
    "word_counts_top10 = word_counts.most_common(11) # 获取前11最高频的词\n",
    "\n",
    "word_counts_top10 = word_counts_top10[1:10]\n",
    "print (word_counts_top10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pylab import *\n",
    "mpl.rcParams['font.sans-serif']=['SimHei']\n",
    "\n",
    "wa=dict(word_counts_top10)\n",
    "plt.bar(wa.keys(),wa.values())\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#从文件导入停用词表\n",
    "stpwrdpath = \"scu_stopwords.txt\"\n",
    "stpwrd_dic = open(stpwrdpath, 'rb')\n",
    "stpwrd_content = stpwrd_dic.read().decode('utf-8')#将停用词表转换为list\n",
    "stpwrdlst = stpwrd_content.splitlines()\n",
    "stpwrd_dic.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (1, 4)\t1\n",
      "  (13, 10)\t1\n",
      "  (17, 10)\t1\n",
      "  (18, 0)\t1\n",
      "  (22, 9)\t1\n",
      "  (33, 3)\t1\n",
      "  (62, 14)\t1\n",
      "  (65, 14)\t1\n",
      "  (72, 15)\t1\n",
      "  (73, 15)\t1\n",
      "  (93, 10)\t1\n",
      "  (95, 3)\t1\n",
      "  (126, 7)\t1\n",
      "  (136, 15)\t1\n",
      "  (141, 0)\t1\n",
      "  (144, 3)\t1\n",
      "  (154, 7)\t1\n",
      "  (159, 10)\t1\n",
      "  (166, 6)\t1\n",
      "  (205, 17)\t1\n",
      "  (212, 17)\t1\n",
      "  (248, 2)\t1\n",
      "  (251, 3)\t1\n",
      "  (253, 7)\t1\n",
      "  (260, 6)\t1\n",
      "  :\t:\n",
      "  (1026, 6)\t1\n",
      "  (1029, 10)\t1\n",
      "  (1037, 8)\t1\n",
      "  (1041, 15)\t1\n",
      "  (1050, 14)\t1\n",
      "  (1051, 17)\t1\n",
      "  (1054, 10)\t1\n",
      "  (1059, 10)\t1\n",
      "  (1076, 5)\t1\n",
      "  (1082, 18)\t1\n",
      "  (1094, 11)\t1\n",
      "  (1113, 5)\t1\n",
      "  (1118, 14)\t1\n",
      "  (1123, 8)\t1\n",
      "  (1129, 6)\t1\n",
      "  (1135, 13)\t1\n",
      "  (1136, 3)\t1\n",
      "  (1159, 15)\t1\n",
      "  (1160, 4)\t1\n",
      "  (1165, 4)\t1\n",
      "  (1178, 6)\t1\n",
      "  (1195, 6)\t1\n",
      "  (1197, 6)\t1\n",
      "  (1198, 15)\t1\n",
      "  (1199, 10)\t1\n",
      "\n",
      "vocabulary list:\n",
      "\n",
      " ['上帝', '事情', '人人生', '人生', '人类', '人类基因', '全人类', '国家', '垃圾', '基因', '孩子', '底线', '意思', '生子', '监狱', '责任', '资格', '资格能力', '问题']\n",
      "\n",
      "vocabulary dic :\n",
      "\n",
      " {'人类': 4, '孩子': 10, '上帝': 0, '基因': 9, '人生': 3, '监狱': 14, '责任': 15, '国家': 7, '全人类': 6, '资格能力': 17, '人人生': 2, '意思': 12, '垃圾': 8, '生子': 13, '问题': 18, '资格': 16, '底线': 11, '事情': 1, '人类基因': 5}\n"
     ]
    }
   ],
   "source": [
    "# 调用sklearn机器学习包进行向量化\n",
    "n_features = 1000 # 指定特征关键词提取最大值\n",
    "tf_vectorizer = CountVectorizer(strip_accents='unicode',# 去除raw document中的重音符号\n",
    "    max_features=n_features,#stop_words='english', # 停用词\n",
    "    stop_words=stpwrdlst,\n",
    "    max_df=0.5, # 阈值如果某个词的document frequence大于max_df，不当作关键词.float，词出现的次数与语料库文档数的百分比，int出现次数\n",
    "    min_df=3) # 如果某个词的document frequence小于min_df，则这个词不会被当作关键词\n",
    "\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(df)\n",
    "print(tf)\n",
    "print('\\nvocabulary list:\\n\\n',tf_vectorizer.get_feature_names())\n",
    "print( '\\nvocabulary dic :\\n\\n',tf_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='online', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=50,\n",
       "                          mean_change_tol=0.001, n_components=5, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=0, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA操作\n",
    "n_topics = 5 # 人为指定划分的主题数\n",
    "lda = LatentDirichletAllocation(n_topics,learning_method='online',max_iter=50,random_state=0 )\n",
    "lda.fit(tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic Nr.1:\n",
      "监狱 5.81 | 资格 3.93 | 资格能力 3.92 | 上帝 2.95 | 全人类 0.2 | 责任 0.2 | 国家 0.2 | 基因 0.2 | 人类 0.2 | 人生 0.2 | \n",
      "\n",
      "Topic Nr.2:\n",
      "垃圾 5.91 | 问题 4.97 | 意思 3.94 | 底线 3.03 | 事情 3.01 | 人人生 2.94 | 全人类 0.2 | 国家 0.2 | 责任 0.2 | 基因 0.2 | \n",
      "\n",
      "Topic Nr.3:\n",
      "孩子 16.78 | 人类基因 4.98 | 生子 3.03 | 全人类 0.2 | 责任 0.2 | 国家 0.2 | 基因 0.2 | 人类 0.2 | 人生 0.2 | 底线 0.2 | \n",
      "\n",
      "Topic Nr.4:\n",
      "人类 18.42 | 人生 6.58 | 基因 4.83 | 全人类 0.2 | 责任 0.2 | 国家 0.2 | 生子 0.2 | 人类基因 0.2 | 底线 0.2 | 事情 0.2 | \n",
      "\n",
      "Topic Nr.5:\n",
      "责任 14.63 | 全人类 14.47 | 国家 6.59 | 基因 0.2 | 人类 0.2 | 人生 0.2 | 生子 0.2 | 人类基因 0.2 | 底线 0.2 | 事情 0.2 | \n"
     ]
    }
   ],
   "source": [
    "# 显示主题关键词函数\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print('\\nTopic Nr.%d:' % int(topic_idx + 1))\n",
    "        print(''.join([feature_names[i] + ' ' + str(round(topic[i], 2)) + ' | ' for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "n_top_words = 10 # 主题输出前20个关键词\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: if you're in the IPython notebook, pyLDAvis.show() is not the best command\n",
      "      to use. Consider using pyLDAvis.display(), or pyLDAvis.enable_notebook().\n",
      "      See more information at http://pyLDAvis.github.io/quickstart.html .\n",
      "\n",
      "You must interrupt the kernel to end this command\n",
      "\n",
      "Serving to http://127.0.0.1:8894/    [Ctrl-C to exit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [30/Jun/2020 22:23:21] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Jun/2020 22:23:21] \"GET /LDAvis.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Jun/2020 22:23:21] \"GET /d3.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/Jun/2020 22:23:21] \"GET /LDAvis.js HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "data = pyLDAvis.sklearn.prepare(lda,tf,tf_vectorizer)\n",
    "\n",
    "#让可视化可以在notebook内显示\n",
    "pyLDAvis.show(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
